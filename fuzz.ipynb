{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 125,
            "source": [
                "import requests\n",
                "import re \n",
                "import string\n",
                "import numpy\n",
                "from bs4 import BeautifulSoup\n",
                "\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 126,
            "source": [
                "import sys\n",
                "\n",
                "class PorterStemmer:\n",
                "    def __init__(self):\n",
                "        \"\"\"The main part of the stemming algorithm starts here.\n",
                "        b is a buffer holding a word to be stemmed. The letters are in b[k0],\n",
                "        b[k0+1] ... ending at b[k]. In fact k0 = 0 in this demo program. k is\n",
                "        readjusted downwards as the stemming progresses. Zero termination is\n",
                "        not in fact used in the algorithm.\n",
                "\n",
                "        Note that only lower case sequences are stemmed. Forcing to lower case\n",
                "        should be done before stem(...) is called.\n",
                "        \"\"\"\n",
                "\n",
                "        self.b = \"\"  # buffer for word to be stemmed\n",
                "        self.k = 0\n",
                "        self.k0 = 0\n",
                "        self.j = 0  # j is a general offset into the string\n",
                "\n",
                "    def cons(self, i):\n",
                "        \"\"\"cons(i) is TRUE <=> b[i] is a consonant.\"\"\"\n",
                "        if (\n",
                "            self.b[i] == \"a\"\n",
                "            or self.b[i] == \"e\"\n",
                "            or self.b[i] == \"i\"\n",
                "            or self.b[i] == \"o\"\n",
                "            or self.b[i] == \"u\"\n",
                "        ):\n",
                "            return 0\n",
                "        if self.b[i] == \"y\":\n",
                "            if i == self.k0:\n",
                "                return 1\n",
                "            else:\n",
                "                return not self.cons(i - 1)\n",
                "        return 1\n",
                "\n",
                "    def m(self):\n",
                "        \"\"\"m() measures the number of consonant sequences between k0 and j.\n",
                "        if c is a consonant sequence and v a vowel sequence, and <..>\n",
                "        indicates arbitrary presence,\n",
                "\n",
                "           <c><v>       gives 0\n",
                "           <c>vc<v>     gives 1\n",
                "           <c>vcvc<v>   gives 2\n",
                "           <c>vcvcvc<v> gives 3\n",
                "           ....\n",
                "        \"\"\"\n",
                "        n = 0\n",
                "        i = self.k0\n",
                "        while 1:\n",
                "            if i > self.j:\n",
                "                return n\n",
                "            if not self.cons(i):\n",
                "                break\n",
                "            i = i + 1\n",
                "        i = i + 1\n",
                "        while 1:\n",
                "            while 1:\n",
                "                if i > self.j:\n",
                "                    return n\n",
                "                if self.cons(i):\n",
                "                    break\n",
                "                i = i + 1\n",
                "            i = i + 1\n",
                "            n = n + 1\n",
                "            while 1:\n",
                "                if i > self.j:\n",
                "                    return n\n",
                "                if not self.cons(i):\n",
                "                    break\n",
                "                i = i + 1\n",
                "            i = i + 1\n",
                "\n",
                "    def vowelinstem(self):\n",
                "        \"\"\"vowelinstem() is TRUE <=> k0,...j contains a vowel\"\"\"\n",
                "        for i in range(self.k0, self.j + 1):\n",
                "            if not self.cons(i):\n",
                "                return 1\n",
                "        return 0\n",
                "\n",
                "    def doublec(self, j):\n",
                "        \"\"\"doublec(j) is TRUE <=> j,(j-1) contain a double consonant.\"\"\"\n",
                "        if j < (self.k0 + 1):\n",
                "            return 0\n",
                "        if self.b[j] != self.b[j - 1]:\n",
                "            return 0\n",
                "        return self.cons(j)\n",
                "\n",
                "    def cvc(self, i):\n",
                "        \"\"\"cvc(i) is TRUE <=> i-2,i-1,i has the form consonant - vowel - consonant\n",
                "        and also if the second c is not w,x or y. this is used when trying to\n",
                "        restore an e at the end of a short  e.g.\n",
                "\n",
                "           cav(e), lov(e), hop(e), crim(e), but\n",
                "           snow, box, tray.\n",
                "        \"\"\"\n",
                "        if (\n",
                "            i < (self.k0 + 2)\n",
                "            or not self.cons(i)\n",
                "            or self.cons(i - 1)\n",
                "            or not self.cons(i - 2)\n",
                "        ):\n",
                "            return 0\n",
                "        ch = self.b[i]\n",
                "        if ch == \"w\" or ch == \"x\" or ch == \"y\":\n",
                "            return 0\n",
                "        return 1\n",
                "\n",
                "    def ends(self, s):\n",
                "        \"\"\"ends(s) is TRUE <=> k0,...k ends with the string s.\"\"\"\n",
                "        length = len(s)\n",
                "        if s[length - 1] != self.b[self.k]:  # tiny speed-up\n",
                "            return 0\n",
                "        if length > (self.k - self.k0 + 1):\n",
                "            return 0\n",
                "        if self.b[self.k - length + 1 : self.k + 1] != s:\n",
                "            return 0\n",
                "        self.j = self.k - length\n",
                "        return 1\n",
                "\n",
                "    def setto(self, s):\n",
                "        \"\"\"setto(s) sets (j+1),...k to the characters in the string s, readjusting k.\"\"\"\n",
                "        length = len(s)\n",
                "        self.b = self.b[: self.j + 1] + s + self.b[self.j + length + 1 :]\n",
                "        self.k = self.j + length\n",
                "\n",
                "    def r(self, s):\n",
                "        \"\"\"r(s) is used further down.\"\"\"\n",
                "        if self.m() > 0:\n",
                "            self.setto(s)\n",
                "\n",
                "    def step1ab(self):\n",
                "        \"\"\"step1ab() gets rid of plurals and -ed or -ing. e.g.\n",
                "\n",
                "           caresses  ->  caress\n",
                "           ponies    ->  poni\n",
                "           ties      ->  ti\n",
                "           caress    ->  caress\n",
                "           cats      ->  cat\n",
                "\n",
                "           feed      ->  feed\n",
                "           agreed    ->  agree\n",
                "           disabled  ->  disable\n",
                "\n",
                "           matting   ->  mat\n",
                "           mating    ->  mate\n",
                "           meeting   ->  meet\n",
                "           milling   ->  mill\n",
                "           messing   ->  mess\n",
                "\n",
                "           meetings  ->  meet\n",
                "        \"\"\"\n",
                "        if self.b[self.k] == \"s\":\n",
                "            if self.ends(\"sses\"):\n",
                "                self.k = self.k - 2\n",
                "            elif self.ends(\"ies\"):\n",
                "                self.setto(\"i\")\n",
                "            elif self.b[self.k - 1] != \"s\":\n",
                "                self.k = self.k - 1\n",
                "        if self.ends(\"eed\"):\n",
                "            if self.m() > 0:\n",
                "                self.k = self.k - 1\n",
                "        elif (self.ends(\"ed\") or self.ends(\"ing\")) and self.vowelinstem():\n",
                "            self.k = self.j\n",
                "            if self.ends(\"at\"):\n",
                "                self.setto(\"ate\")\n",
                "            elif self.ends(\"bl\"):\n",
                "                self.setto(\"ble\")\n",
                "            elif self.ends(\"iz\"):\n",
                "                self.setto(\"ize\")\n",
                "            elif self.doublec(self.k):\n",
                "                self.k = self.k - 1\n",
                "                ch = self.b[self.k]\n",
                "                if ch == \"l\" or ch == \"s\" or ch == \"z\":\n",
                "                    self.k = self.k + 1\n",
                "            elif self.m() == 1 and self.cvc(self.k):\n",
                "                self.setto(\"e\")\n",
                "\n",
                "    def step1c(self):\n",
                "        \"\"\"step1c() turns terminal y to i when there is another vowel in the stem.\"\"\"\n",
                "        if self.ends(\"y\") and self.vowelinstem():\n",
                "            self.b = self.b[: self.k] + \"i\" + self.b[self.k + 1 :]\n",
                "\n",
                "    def step2(self):\n",
                "        \"\"\"step2() maps double suffices to single ones.\n",
                "        so -ization ( = -ize plus -ation) maps to -ize etc. note that the\n",
                "        string before the suffix must give m() > 0.\n",
                "        \"\"\"\n",
                "        if self.b[self.k - 1] == \"a\":\n",
                "            if self.ends(\"ational\"):\n",
                "                self.r(\"ate\")\n",
                "            elif self.ends(\"tional\"):\n",
                "                self.r(\"tion\")\n",
                "        elif self.b[self.k - 1] == \"c\":\n",
                "            if self.ends(\"enci\"):\n",
                "                self.r(\"ence\")\n",
                "            elif self.ends(\"anci\"):\n",
                "                self.r(\"ance\")\n",
                "        elif self.b[self.k - 1] == \"e\":\n",
                "            if self.ends(\"izer\"):\n",
                "                self.r(\"ize\")\n",
                "        elif self.b[self.k - 1] == \"l\":\n",
                "            if self.ends(\"bli\"):\n",
                "                self.r(\"ble\")  # --DEPARTURE--\n",
                "            # To match the published algorithm, replace this phrase with\n",
                "            #   if self.ends(\"abli\"):      self.r(\"able\")\n",
                "            elif self.ends(\"alli\"):\n",
                "                self.r(\"al\")\n",
                "            elif self.ends(\"entli\"):\n",
                "                self.r(\"ent\")\n",
                "            elif self.ends(\"eli\"):\n",
                "                self.r(\"e\")\n",
                "            elif self.ends(\"ousli\"):\n",
                "                self.r(\"ous\")\n",
                "        elif self.b[self.k - 1] == \"o\":\n",
                "            if self.ends(\"ization\"):\n",
                "                self.r(\"ize\")\n",
                "            elif self.ends(\"ation\"):\n",
                "                self.r(\"ate\")\n",
                "            elif self.ends(\"ator\"):\n",
                "                self.r(\"ate\")\n",
                "        elif self.b[self.k - 1] == \"s\":\n",
                "            if self.ends(\"alism\"):\n",
                "                self.r(\"al\")\n",
                "            elif self.ends(\"iveness\"):\n",
                "                self.r(\"ive\")\n",
                "            elif self.ends(\"fulness\"):\n",
                "                self.r(\"ful\")\n",
                "            elif self.ends(\"ousness\"):\n",
                "                self.r(\"ous\")\n",
                "        elif self.b[self.k - 1] == \"t\":\n",
                "            if self.ends(\"aliti\"):\n",
                "                self.r(\"al\")\n",
                "            elif self.ends(\"iviti\"):\n",
                "                self.r(\"ive\")\n",
                "            elif self.ends(\"biliti\"):\n",
                "                self.r(\"ble\")\n",
                "        elif self.b[self.k - 1] == \"g\":  # --DEPARTURE--\n",
                "            if self.ends(\"logi\"):\n",
                "                self.r(\"log\")\n",
                "        # To match the published algorithm, delete this phrase\n",
                "\n",
                "    def step3(self):\n",
                "        \"\"\"step3() dels with -ic-, -full, -ness etc. similar strategy to step2.\"\"\"\n",
                "        if self.b[self.k] == \"e\":\n",
                "            if self.ends(\"icate\"):\n",
                "                self.r(\"ic\")\n",
                "            elif self.ends(\"ative\"):\n",
                "                self.r(\"\")\n",
                "            elif self.ends(\"alize\"):\n",
                "                self.r(\"al\")\n",
                "        elif self.b[self.k] == \"i\":\n",
                "            if self.ends(\"iciti\"):\n",
                "                self.r(\"ic\")\n",
                "        elif self.b[self.k] == \"l\":\n",
                "            if self.ends(\"ical\"):\n",
                "                self.r(\"ic\")\n",
                "            elif self.ends(\"ful\"):\n",
                "                self.r(\"\")\n",
                "        elif self.b[self.k] == \"s\":\n",
                "            if self.ends(\"ness\"):\n",
                "                self.r(\"\")\n",
                "\n",
                "    def step4(self):\n",
                "        \"\"\"step4() takes off -ant, -ence etc., in context <c>vcvc<v>.\"\"\"\n",
                "        if self.b[self.k - 1] == \"a\":\n",
                "            if self.ends(\"al\"):\n",
                "                pass\n",
                "            else:\n",
                "                return\n",
                "        elif self.b[self.k - 1] == \"c\":\n",
                "            if self.ends(\"ance\"):\n",
                "                pass\n",
                "            elif self.ends(\"ence\"):\n",
                "                pass\n",
                "            else:\n",
                "                return\n",
                "        elif self.b[self.k - 1] == \"e\":\n",
                "            if self.ends(\"er\"):\n",
                "                pass\n",
                "            else:\n",
                "                return\n",
                "        elif self.b[self.k - 1] == \"i\":\n",
                "            if self.ends(\"ic\"):\n",
                "                pass\n",
                "            else:\n",
                "                return\n",
                "        elif self.b[self.k - 1] == \"l\":\n",
                "            if self.ends(\"able\"):\n",
                "                pass\n",
                "            elif self.ends(\"ible\"):\n",
                "                pass\n",
                "            else:\n",
                "                return\n",
                "        elif self.b[self.k - 1] == \"n\":\n",
                "            if self.ends(\"ant\"):\n",
                "                pass\n",
                "            elif self.ends(\"ement\"):\n",
                "                pass\n",
                "            elif self.ends(\"ment\"):\n",
                "                pass\n",
                "            elif self.ends(\"ent\"):\n",
                "                pass\n",
                "            else:\n",
                "                return\n",
                "        elif self.b[self.k - 1] == \"o\":\n",
                "            if self.ends(\"ion\") and (self.b[self.j] == \"s\" or self.b[self.j] == \"t\"):\n",
                "                pass\n",
                "            elif self.ends(\"ou\"):\n",
                "                pass\n",
                "            # takes care of -ous\n",
                "            else:\n",
                "                return\n",
                "        elif self.b[self.k - 1] == \"s\":\n",
                "            if self.ends(\"ism\"):\n",
                "                pass\n",
                "            else:\n",
                "                return\n",
                "        elif self.b[self.k - 1] == \"t\":\n",
                "            if self.ends(\"ate\"):\n",
                "                pass\n",
                "            elif self.ends(\"iti\"):\n",
                "                pass\n",
                "            else:\n",
                "                return\n",
                "        elif self.b[self.k - 1] == \"u\":\n",
                "            if self.ends(\"ous\"):\n",
                "                pass\n",
                "            else:\n",
                "                return\n",
                "        elif self.b[self.k - 1] == \"v\":\n",
                "            if self.ends(\"ive\"):\n",
                "                pass\n",
                "            else:\n",
                "                return\n",
                "        elif self.b[self.k - 1] == \"z\":\n",
                "            if self.ends(\"ize\"):\n",
                "                pass\n",
                "            else:\n",
                "                return\n",
                "        else:\n",
                "            return\n",
                "        if self.m() > 1:\n",
                "            self.k = self.j\n",
                "\n",
                "    def step5(self):\n",
                "        \"\"\"step5() removes a final -e if m() > 1, and changes -ll to -l if\n",
                "        m() > 1.\n",
                "        \"\"\"\n",
                "        self.j = self.k\n",
                "        if self.b[self.k] == \"e\":\n",
                "            a = self.m()\n",
                "            if a > 1 or (a == 1 and not self.cvc(self.k - 1)):\n",
                "                self.k = self.k - 1\n",
                "        if self.b[self.k] == \"l\" and self.doublec(self.k) and self.m() > 1:\n",
                "            self.k = self.k - 1\n",
                "\n",
                "    def stem(self, p):\n",
                "        \"\"\"In stem(p,i,j), p is a char pointer, and the string to be stemmed\n",
                "        is from p[i] to p[j] inclusive. Typically i is zero and j is the\n",
                "        offset to the last character of a string, (p[j+1] == '\\0'). The\n",
                "        stemmer adjusts the characters p[i] ... p[j] and returns the new\n",
                "        end-point of the string, k. Stemming never increases word length, so\n",
                "        i <= k <= j. To turn the stemmer into a module, declare 'stem' as\n",
                "        extern, and delete the remainder of this file.\n",
                "        \"\"\"\n",
                "        # copy the parameters into statics\n",
                "        self.b = p\n",
                "        self.k = len(p) - 1\n",
                "        self.k0 = 0\n",
                "        if self.k <= self.k0 + 1:\n",
                "            return self.b  # --DEPARTURE--\n",
                "\n",
                "        # With this line, strings of length 1 or 2 don't go through the\n",
                "        # stemming process, although no mention is made of this in the\n",
                "        # published algorithm. Remove the line to match the published\n",
                "        # algorithm.\n",
                "\n",
                "        self.step1ab()\n",
                "        self.step1c()\n",
                "        self.step2()\n",
                "        self.step3()\n",
                "        self.step4()\n",
                "        self.step5()\n",
                "        return self.b[self.k0 : self.k + 1]\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 140,
            "source": [
                "def stemAll(tokenizers):\n",
                "    p = PorterStemmer()\n",
                "    stems = []\n",
                "    for str in tokenizers:\n",
                "        str1= p.stem(str)\n",
                "        if str1 != str:\n",
                "            stems.append(str1)\n",
                "    return stems\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 165,
            "source": [
                "def defaultwordlist():\n",
                "    defaultwordlists = []\n",
                "    with open('file1', 'r',encoding='utf-8') as f:\n",
                "        for line in f.readlines():\n",
                "            defaultwordlists.append(line.strip())\n",
                "    return defaultwordlists\n",
                "\n",
                "def tokenize(target_url):\n",
                "    html = requests.get(target_url)\n",
                "    soup = BeautifulSoup(html.text, \"html.parser\")\n",
                "\n",
                "    text = soup.get_text()\n",
                "    tokenizers = []\n",
                "    # delete empty lines\n",
                "    lines = text.split(\"\\n\")\n",
                "    modified_lines = [line for line in lines if line.strip() != \"\"]\n",
                "    no_empty_lines = \"\"\n",
                "    for line in modified_lines:\n",
                "        no_empty_lines += line + \" \"\n",
                "\n",
                "    # delete symbols\n",
                "    no_empty_lines = re.sub(r'[^\\w\\s]','',no_empty_lines)\n",
                "# write pure txt\n",
                "    tokenizers = re.split(r\"[ ]+\",no_empty_lines.strip().lower())\n",
                "    return tokenizers\n",
                "    \n",
                "def tokenize_split(target_url):\n",
                "    html = requests.get(target_url)\n",
                "    soup = BeautifulSoup(html.text, \"html.parser\")\n",
                "\n",
                "    text = soup.get_text()\n",
                "\n",
                "    # delete empty lines\n",
                "    lines = text.split(\"\\n\")\n",
                "    modified_lines = [line for line in lines if line.strip() != \"\"]\n",
                "    seq_list=[]\n",
                "\n",
                "    for pharse in modified_lines:\n",
                "        pharse = re.sub(r'[^\\w\\s]','',pharse.strip())\n",
                "        pharse = re.split(r\"[ ]+\",pharse)\n",
                "        if len(pharse) < 5 and len(pharse) > 1:\n",
                "            seq_list.append(pharse)\n",
                "    return seq_list\n",
                "\n",
                "\n",
                "\n",
                "def tokenize_url(target_url):\n",
                "    html = requests.get(target_url)\n",
                "    soup = BeautifulSoup(html.text, \"html.parser\")\n",
                "    tokenizers_url = []\n",
                "    # write urls\n",
                "    for link in soup.find_all('a'):\n",
                "        link_str = str(link.get('href'))\n",
                "        if link_str.startswith(target_url):\n",
                "            tokenizers_url.append(link_str)\n",
                "        else:\n",
                "            pass\n",
                "    return tokenizers_url\n",
                "\n",
                "\n",
                "\n",
                "def prefix(target_url):\n",
                "    html = requests.get(target_url)\n",
                "    soup = BeautifulSoup(html.text, \"html.parser\")\n",
                "    soup_prettify = soup.prettify()\n",
                "    sample_str = []\n",
                "    # write urls\n",
                "    with open('link1.txt', 'w') as f:\n",
                "        for link in soup.find_all('a'):\n",
                "            link_str = str(link.get('href'))\n",
                "\n",
                "            if link_str.startswith(target_url):\n",
                "                f.write(link_str + '\\n')\n",
                "                sample_str += link_str.split(\"/\")\n",
                "            else:\n",
                "                pass\n",
                "\n",
                "    token_dash = []\n",
                "    for x in sample_str:\n",
                "        toke_split= tokenize_split(target_url)\n",
                "        for tokens in toke_split:\n",
                "            Token = \"\"\n",
                "            for token in tokens:\n",
                "                Token += token + \"-\"\n",
                "            token_dash.append(Token[0:-1])\n",
                "            #token_dash.append(Token[0:-1].lower())\n",
                "        break\n",
                "    for x in sample_str:\n",
                "        toke_split= tokenize_split(target_url)\n",
                "        for tokens in toke_split:\n",
                "            Token = \"\"\n",
                "            for token in tokens:\n",
                "                Token += token + \"_\"\n",
                "            token_dash.append(Token[0:-1])\n",
                "            #token_dash.append(Token[0:-1].lower())\n",
                "        break\n",
                "    return token_dash\n",
                "\n",
                "#url = \"https://uwaterloo.ca/coronavirus/\"\n",
                "#print(tokenize(url))\n",
                "#print(tokenize_split(url))\n",
                "#print(tokenize_url(url))\n",
                "#print(prefix(url))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 166,
            "source": [
                "#default wordlists\n",
                "def defaultfuzz(url, defaultwordlists):\n",
                "    for str in defaultwordlists:\n",
                "        fuzzurl = url + \"/\" + str\n",
                "        response = requests.get(fuzzurl)\n",
                "        if(response.status_code == 200):\n",
                "            print(fuzzurl)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 167,
            "source": [
                "def lexicon1(tokenlist,multitokenlist,defaultlist):\n",
                "    lexicons = {}\n",
                "    for word in tokenlist:\n",
                "        if word in lexicons:\n",
                "            lexicons[word.lower()]+=1\n",
                "        else:\n",
                "            lexicons[word.lower()] = 1\n",
                "    for word in defaultlist:\n",
                "        if word in lexicons:\n",
                "            lexicons[word.lower()]+=1\n",
                "        else:\n",
                "            lexicons[word.lower()] = 1\n",
                "    for word in multitokenlist:\n",
                "        if word in lexicons:\n",
                "            lexicons[word.lower()]+=5\n",
                "        else:\n",
                "            lexicons[word.lower()] = 5\n",
                "    lexicons = sorted(lexicons.items(),key = lambda x:x[1],reverse = True)\n",
                "    #print(lexicons)\n",
                "    return lexicons\n",
                "\n",
                "def lexiconstem(stemlist):\n",
                "    lexicons = {}\n",
                "    for word in stemlist:\n",
                "        if word in lexicons:\n",
                "            lexicons[word.lower()]+=1\n",
                "        else:\n",
                "            lexicons[word.lower()] = 1\n",
                "    lexicons = sorted(lexicons.items(),key = lambda x:x[1],reverse = True)\n",
                "    #print(lexicons)\n",
                "    return lexicons\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 168,
            "source": [
                "def stemfuzz(url, stems):\n",
                "    for str in stems:\n",
                "        fuzzurl = url + \"/\" + str[0]\n",
                "        response = requests.get(fuzzurl)\n",
                "        if(response.status_code == 200):\n",
                "            print(fuzzurl)\n",
                "    for str in stems:\n",
                "        fuzzurl = url + \"/\" + str[0] + \"e\"\n",
                "        response = requests.get(fuzzurl)\n",
                "        if(response.status_code == 200):\n",
                "            print(fuzzurl)\n",
                "    for str in stems:\n",
                "        fuzzurl = url + \"/\" + str[0] + \"n\"\n",
                "        response = requests.get(fuzzurl)\n",
                "        if(response.status_code == 200):\n",
                "            print(fuzzurl)\n",
                "    for str in stems:\n",
                "        fuzzurl = url + \"/\" + str[0] + \"ize\"\n",
                "        response = requests.get(fuzzurl)\n",
                "        if(response.status_code == 200):\n",
                "            print(fuzzurl)\n",
                "    for str in stems:\n",
                "        fuzzurl = url + \"/\" + str[0] + \"s\"\n",
                "        response = requests.get(fuzzurl)\n",
                "        if(response.status_code == 200):\n",
                "            print(fuzzurl)\n",
                "    for str in stems:\n",
                "        fuzzurl = url + \"/\" + str[0] + \"y\"\n",
                "        response = requests.get(fuzzurl)\n",
                "        if(response.status_code == 200):\n",
                "            print(fuzzurl)\n",
                "\n",
                "#single word without stemming\n",
                "def textfuzz(url, tokenizers):\n",
                "    for str in tokenizers:\n",
                "        fuzzurl = url + \"/\" + str[0]\n",
                "        response = requests.get(fuzzurl)\n",
                "        if(response.status_code == 200):\n",
                "            print(fuzzurl)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 178,
            "source": [
                "\n",
                "url = \"https://uwaterloo.ca/coronavirus\"\n",
                "url = \"https://lib.uwaterloo.ca\"\n",
                "response = requests.get(url)\n",
                "if(response.status_code == 200):\n",
                "    print(\"hehe\")\n",
                "print(response.status_code)\n",
                "#print(response.text)\n",
                "httpstr = response.text\n",
                "#deprive all punctuation\n",
                "###############tokenizers are all words############\n",
                "tokenlist = tokenize(url)\n",
                "#print(tokenlist)\n",
                "multitokenlist = prefix(url)\n",
                "#print(multitokenizers)\n",
                "stemlist = stemAll(tokenlist)\n",
                "defaultlist = defaultwordlist()\n",
                "finalwordlist = lexicon1(tokenlist,multitokenlist,defaultlist)\n",
                "finalstemlist = lexiconstem(stemlist)\n",
                "print(finalstemlist)\n",
                "################start fuzz##########################\n",
                "textfuzz(url, finalwordlist)\n",
                "stemfuzz(url, finalstemlist)\n",
                "#textfuzz(url, finalwordlist)\n",
                "\n",
                "\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "hehe\n",
                        "200\n",
                        "[('servic', 20), ('centr', 20), ('offic', 18), ('univers', 11), ('associ', 8), ('academ', 7), ('campu', 7), ('inform', 6), ('commun', 6), ('institut', 6), ('intern', 6), ('relat', 5), ('learn', 5), ('develop', 5), ('admiss', 4), ('faculti', 4), ('resourc', 4), ('extend', 4), ('technolog', 4), ('profession', 4), ('initi', 4), ('plan', 4), ('student', 3), ('access', 3), ('graduat', 3), ('hous', 3), ('excel', 3), ('educ', 3), ('privaci', 3), ('scienc', 3), ('equiti', 3), ('indigen', 3), ('warrior', 2), ('undergradu', 2), ('advanc', 2), ('itm', 2), ('teach', 2), ('manag', 2), ('right', 2), ('deputi', 2), ('assist', 2), ('languag', 2), ('financ', 2), ('financi', 2), ('studi', 2), ('postdoctor', 2), ('affair', 2), ('inclus', 2), ('system', 2), ('analysi', 2), ('sustain', 2), ('watsaf', 2), ('take', 2), ('websit', 1), ('futur', 1), ('integr', 1), ('reviewsqu', 1), ('assur', 1), ('galleri', 1), ('vicepresid', 1), ('athlet', 1), ('recreat', 1), ('audiovisu', 1), ('bombshelt', 1), ('brubach', 1), ('respons', 1), ('well', 1), ('cater', 1), ('store', 1), ('polici', 1), ('cap', 1), ('clinic', 1), ('appli', 1), ('confer', 1), ('cooper', 1), ('counsel', 1), ('creativ', 1), ('cybersecur', 1), ('distanc', 1), ('edg', 1), ('employe', 1), ('profici', 1), ('elp', 1), ('fed', 1), ('bu', 1), ('divers', 1), ('govern', 1), ('gradventur', 1), ('instruct', 1), ('new', 1), ('intramur', 1), ('immigr', 1), ('librari', 1), ('market', 1), ('strateg', 1), ('mate', 1), ('microscopi', 1), ('confoc', 1), ('occup', 1), ('offcampu', 1), ('presid', 1), ('organiz', 1), ('park', 1), ('parttim', 1), ('person', 1), ('disabl', 1), ('oper', 1), ('polic', 1), ('solut', 1), ('procur', 1), ('provost', 1), ('advocaci', 1), ('solidar', 1), ('rais', 1), ('registrar', 1), ('retire', 1), ('safeti', 1), ('formerli', 1), ('recognit', 1), ('award', 1), ('accommod', 1), ('telecommun', 1), ('telephon', 1), ('theatr', 1), ('record', 1), ('fit', 1), ('veloc', 1), ('visitor', 1), ('volunt', 1), ('waterloowork', 1), ('complex', 1), ('innov', 1), ('women', 1), ('write', 1), ('us', 1), ('book', 1), ('avenu', 1), ('map', 1), ('direct', 1), ('career', 1), ('youtub', 1), ('directori', 1), ('acknowledg', 1), ('tradit', 1), ('territori', 1), ('haudenosaune', 1), ('peopl', 1), ('situat', 1), ('grant', 1), ('nation', 1), ('includ', 1), ('mile', 1), ('activ', 1), ('reconcili', 1), ('campus', 1), ('build', 1), ('central', 1)]\n"
                    ]
                },
                {
                    "output_type": "error",
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipykernel_27185/3149498396.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinalstemlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m################start fuzz##########################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtextfuzz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalwordlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mstemfuzz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalstemlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#textfuzz(url, finalwordlist)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/tmp/ipykernel_27185/1052641207.py\u001b[0m in \u001b[0;36mtextfuzz\u001b[0;34m(url, tokenizers)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mfuzzurl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuzzurl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuzzurl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/lib/python3/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/lib/python3/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/lib/python3/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/lib/python3/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/lib/python3/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    666\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    994\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             conn = connection.create_connection(\n\u001b[0m\u001b[1;32m    160\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             )\n",
                        "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Generate the stemmer function here"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.10 64-bit"
        },
        "interpreter": {
            "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}